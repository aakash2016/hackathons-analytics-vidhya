{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7920, 3)\n",
      "(1953, 2)\n"
     ]
    }
   ],
   "source": [
    "## Importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train = pd.read_csv('trainl.csv')\n",
    "test = pd.read_csv('testl.csv')\n",
    "print(train.shape); print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5894\n",
       "1    2026\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is typical sentiment Analysis problem.\n",
    "# Customer Tweets related to tech firms who are manufacturers of mobiles, laptops are given to us.\n",
    "# The task is to determine tweets which have negative sentiments towards such companies or products.\n",
    "train.label.value_counts() #Most of the tweets have positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.isna().sum()\n",
    "## Clearly there are no missing values.\n",
    "## Data Preprocessing\n",
    "## Not using deep learning models using simple ml algorithm - Logistic Regression.\n",
    "# And so we will simply use frequency based embeddings loke tfidf or count vectorizer.\n",
    "def clean_text(text):\n",
    "    # firstly put all the texts in lower cases\n",
    "    text = text.lower()\n",
    "    text = text.replace('$&@*#', 'bakwas')\n",
    "    text = text.replace('f**k', 'fuck')\n",
    "    text = text.replace('@$$hole', 'asshole')\n",
    "    text = text.replace('f#%*king', 'fucking')\n",
    "    text = text.replace(':@', 'bakwas')\n",
    "    return text\n",
    "train['tweet']=train['tweet'].apply(lambda x: clean_text(x))\n",
    "test['tweet']=test['tweet'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since twitter ID can be '@' followed by some alphanumeric we need to remove them.\n",
    "# Because they are just ID's and will play any role in determining the sentiments.\n",
    "def remove_user(text):\n",
    "    r = re.findall('@[\\w]*', text)\n",
    "    for i in r:\n",
    "        text = re.sub(i, '', text)\n",
    "    return text    \n",
    "train.tweet = train.tweet.apply(lambda x: remove_user(x))\n",
    "test.tweet = test.tweet.apply(lambda x: remove_user(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarly there are many URL's which we need to remove as they wont play any role in sentiments.\n",
    "def remove_url(text):\n",
    "    text = re.sub('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', text)\n",
    "    return text   \n",
    "train.tweet = train.tweet.apply(lambda x: remove_url(x))\n",
    "test.tweet = test.tweet.apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will split our training data into train and validation so that we can do proper regularisation.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train['tweet'], train['label'], test_size = 0.1,\n",
    "                                                      random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819672131147541\n"
     ]
    }
   ],
   "source": [
    "## Part1 -- using count vectoriser and Naive Bayes Algorithm.\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = MultinomialNB(alpha = 0.0925)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_valid))\n",
    "## Clearly our submissions are evaluated on the basis of F1Score\n",
    "print(f1_score(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8253275109170305\n"
     ]
    }
   ],
   "source": [
    "## Part2 -- using tfidf vectorizer and Naive Bayes Algorithm.\n",
    "tfvect = TfidfVectorizer().fit(X_train)\n",
    "X_train_vectorized = tfvect.transform(X_train)\n",
    "\n",
    "model = MultinomialNB(alpha = 0.0955)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(tfvect.transform(X_valid))\n",
    "print(f1_score(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8301886792452831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aakash/miniconda3/envs/chatbot/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "## Part3 -- using count vectoriser and Logistic Regression Algorithm.\n",
    "vect = CountVectorizer(min_df=2, ngram_range=(1,3)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression(C = 1.6, solver = 'sag')\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_valid))\n",
    "print(f1_score(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part4 -- using tfidf vectorizer and Logistic Regression Algorithm.\n",
    "## Word Level tf idf vectorizer.\n",
    "\n",
    "text = pd.concat([train.tweet, test.tweet])\n",
    "Tfword_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='unicode',analyzer='word',ngram_range=(1, 3),max_features=10000).fit(text)\n",
    "word_train_vectorized = Tfword_vectorizer.transform(X_train)\n",
    "word_valid_vectorized = Tfword_vectorizer.transform(X_valid)\n",
    "word_test_vectorized = Tfword_vectorizer.transform(test.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Character level tf idf vectoriser.\n",
    "Tfchar_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='unicode',analyzer='char',ngram_range=(1, 15),max_features=50000).fit(text)\n",
    "char_train_vectorized = Tfchar_vectorizer.transform(X_train)\n",
    "char_valid_vectorized = Tfchar_vectorizer.transform(X_valid)\n",
    "char_test_vectorized = Tfchar_vectorizer.transform(test.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Horizontally stacking the tf idf vectorizers.\n",
    "train_features = hstack([char_train_vectorized, word_train_vectorized])\n",
    "valid_features = hstack([char_valid_vectorized, word_valid_vectorized])\n",
    "test_features = hstack([char_test_vectorized, word_test_vectorized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8364485981308412\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=300,C=2.0,solver='sag')\n",
    "model.fit(train_features, y_train)\n",
    "predictions = model.predict(valid_features)\n",
    "pred_y = model.predict(test_features)\n",
    "print(f1_score(y_valid, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
